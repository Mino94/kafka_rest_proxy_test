{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c01e3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-12 14:06:16.035090\n",
      "2021-10-12\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "\n",
    "temp_date = dt.datetime.now()\n",
    "print(temp_date)\n",
    "post_day = str(temp_date).split(' ')\n",
    "print(post_day[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa647237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_post_date(post_date):\n",
    "    temp_date = post_date\n",
    "\n",
    "    if '일전' in temp_date:\n",
    "        post_day_list = re.findall('\\d+', temp_date)\n",
    "        temp_int = int(post_day_list[0])\n",
    "        post_day = today - timedelta(days=temp_int)\n",
    "        date_hour = post_day.strftime('%Y.%m.%d %H')\n",
    "        tmp_day = post_day.strftime('%Y-%m-%d %H:%M')\n",
    "        tmp_date = time.mktime(datetime.datetime.strptime(tmp_day, '%Y-%m-%d %H:%M').timetuple())\n",
    "        date = datetime.datetime.fromtimestamp(tmp_date)\n",
    "\n",
    "    elif '시간전' in temp_date:\n",
    "        post_day_list = re.findall('\\d+', temp_date)\n",
    "        temp_int = int(post_day_list[0])\n",
    "        post_day = today - timedelta(hours=temp_int)\n",
    "        date_hour = post_day.strftime('%Y.%m.%d %H')\n",
    "        tmp_day = post_day.strftime('%Y-%m-%d %H:%M')\n",
    "        tmp_date = time.mktime(datetime.datetime.strptime(tmp_day, '%Y-%m-%d %H:%M').timetuple())\n",
    "        date = datetime.datetime.fromtimestamp(tmp_date)\n",
    "\n",
    "    elif '분전' in temp_date:\n",
    "        post_day_list = re.findall('\\d+', temp_date)\n",
    "        temp_int = int(post_day_list[0])\n",
    "        post_day = today - timedelta(minutes=temp_int)\n",
    "        date_hour = post_day.strftime('%Y.%m.%d %H')\n",
    "        tmp_day = post_day.strftime('%Y-%m-%d %H:%M')\n",
    "        tmp_date = time.mktime(datetime.datetime.strptime(tmp_day, '%Y-%m-%d %H:%M').timetuple())\n",
    "        date = datetime.datetime.fromtimestamp(tmp_date)\n",
    "\n",
    "    elif '초전' in temp_date:\n",
    "        post_day_list = re.findall('\\d+', temp_date)\n",
    "        temp_int = int(post_day_list[0])\n",
    "        post_day = today - timedelta(seconds=temp_int)\n",
    "        date_hour = post_day.strftime('%Y.%m.%d %H')\n",
    "        tmp_day = post_day.strftime('%Y-%m-%d %H:%M')\n",
    "        tmp_date = time.mktime(datetime.datetime.strptime(tmp_day, '%Y-%m-%d %H:%M').timetuple())\n",
    "        date = datetime.datetime.fromtimestamp(tmp_date)\n",
    "\n",
    "    elif '조금전' in temp_date:\n",
    "        temp_int = 30\n",
    "        post_day = today - timedelta(seconds=temp_int)\n",
    "        date_hour = post_day.strftime('%Y.%m.%d %H')\n",
    "        tmp_day = post_day.strftime('%Y-%m-%d %H:%M')\n",
    "        tmp_date = time.mktime(datetime.datetime.strptime(tmp_day, '%Y-%m-%d %H:%M').timetuple())\n",
    "        date = datetime.datetime.fromtimestamp(tmp_date)\n",
    "\n",
    "    elif '어제' in temp_date:\n",
    "        post_day = today - timedelta(hours=24)\n",
    "        date_hour = post_day.strftime('%Y.%m.%d %H')\n",
    "        tmp_day = post_day.strftime('%Y-%m-%d %H:%M')\n",
    "        tmp_date = time.mktime(datetime.datetime.strptime(tmp_day, '%Y-%m-%d %H:%M').timetuple())\n",
    "        date = datetime.datetime.fromtimestamp(tmp_date)\n",
    "\n",
    "    elif '오늘' in temp_date:\n",
    "        post_day = datetime.datetime.now()\n",
    "        date_hour = post_day.strftime('%Y.%m.%d %H')\n",
    "        tmp_day = post_day.strftime('%Y-%m-%d %H:%M')\n",
    "        tmp_date = time.mktime(datetime.datetime.strptime(tmp_day, '%Y-%m-%d %H:%M').timetuple())\n",
    "        date = datetime.datetime.fromtimestamp(tmp_date)\n",
    "\n",
    "    elif len(temp_date) == 19:\n",
    "        post_day = datetime.datetime.strptime(temp_date, '%Y. %m. %d. %H:%M')\n",
    "        date_hour = post_day.strftime('%Y.%m.%d %H')\n",
    "        tmp_day = post_day.strftime('%Y-%m-%d %H:%M')\n",
    "        tmp_date = time.mktime(datetime.datetime.strptime(tmp_day, '%Y-%m-%d %H:%M').timetuple())\n",
    "        date = datetime.datetime.fromtimestamp(tmp_date)\n",
    "\n",
    "    else:\n",
    "        post_day = datetime.datetime.strptime(temp_date, '%Y.%m.%d %H:%M')\n",
    "        date_hour = post_day.strftime('%Y.%m.%d %H')\n",
    "        tmp_day = post_day.strftime('%Y-%m-%d %H:%M')\n",
    "        tmp_date = time.mktime(datetime.datetime.strptime(tmp_day, '%Y-%m-%d %H:%M').timetuple())\n",
    "        date = datetime.datetime.fromtimestamp(tmp_date)\n",
    "    return date, date_hour\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "HEADERS_LIST = \"\"\"Mozilla/5.0 Slackware/13.37 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/11.0.696.50\n",
    "Mozilla/5.0 Slackware/13.37 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/12.0.742.91\"\"\".split('\\n')\n",
    "HEADERS_LIST = [x.strip() for x in HEADERS_LIST]\n",
    "agent = random.choice(HEADERS_LIST)\n",
    "headers = {\"User-Agent\": \"{}\".format(agent)}\n",
    "\n",
    "\n",
    "# 감정분석 api 변수 저장\n",
    "emo_url = 'http://172.16.114.47:80/model-engine/social_senti_keyword/'\n",
    "\n",
    "DRIVER_PATH = '/Users/kimminho/chromedriver'\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "\n",
    "keyword = \"이재명\"\n",
    "startDate = \"20211004140224\"\n",
    "endDate = \"20211005140224\"\n",
    "\n",
    "for pageNo in range(1, 80):\n",
    "    print(pageNo)\n",
    "    target_url = f\"https://search.daum.net/search?w=blog&enc=utf8&q={keyword}&f=section&SA=daumsec&period=d&p={pageNo}&f=sectionsd={startDate}&ed={endDate}\"\n",
    "    driver.get(target_url)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    \n",
    "    if soup.find(\"span\", attrs={\"class\":\"btn_page btn_next\"}):\n",
    "        print(\"더 이상 페이지가 없습니다.\")\n",
    "        break\n",
    "        \n",
    "    if soup.find(\"div\", attrs={\"id\": \"noResult\"}):\n",
    "        print(\"검색결과 없음\", target_url)    \n",
    "        #break\n",
    "\n",
    "    posts = soup.find(\"ul\", attrs={\"class\": \"list_info mg_cont clear\"}).find_all(\"li\")\n",
    "    for post in posts:\n",
    "        \n",
    "        \n",
    "        post_link = \"\"\n",
    "        \n",
    "        post_title = post.find(\"a\", attrs={\"class\": \"f_link_b\"}).get_text()\n",
    "        post_link = post.find(\"a\", attrs={\"class\": \"f_link_b\"})['href']\n",
    "\n",
    "\n",
    "        blog_name_list = post.find_all(\"span\", attrs={\"class\": \"f_l\"})\n",
    "        blog_name = blog_name_list[0].find(\"a\", attrs={\"class\": \"f_nb\"}).get_text()\n",
    "\n",
    "        try:\n",
    "            res = requests.get(post_link, headers=headers)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            if soup.find('div', {'class': 'content_error'}):\n",
    "                print(\"해당 게시물이 존재하지 않습니다.\")\n",
    "                break\n",
    "\n",
    "            if soup.find('div', {'id':'article-reply'}):\n",
    "                print(\"article-reply:\",post_title, post_link)\n",
    "                try:\n",
    "                    comment_list = [p for p in soup.find_all('div', {\"class\":\"box-content\"})]\n",
    "\n",
    "                    for tag in comment_list:\n",
    "                        user_name = tag.find('strong').text\n",
    "                        body = tag.find('p').text.replace(\"\\r\\n\", \" \")\n",
    "\n",
    "                        temp_date_li = tag.find('span').text\n",
    "                        temp_date = temp_date_li.split(' ')[0]+' '+temp_date_li.split(' ')[1]\n",
    "\n",
    "                        date = convert_post_date(temp_date)[0]\n",
    "                        dt_hour = convert_post_date(temp_date)[1]\n",
    "\n",
    "                        temp_id = str(post_title + str(date) + user_name)\n",
    "                        temp_id = temp_id.encode('utf-8')\n",
    "                        h = hashlib.new('md5')\n",
    "                        h.update(temp_id)\n",
    "                        _id = h.hexdigest()  \n",
    "\n",
    "                        # 감정분석 추가\n",
    "                        try:\n",
    "                            emo_response = requests.post(emo_url, json={'text': [comment]}, timeout=5)\n",
    "                            emotions = emo_response.json()\n",
    "                            source_emo = emotions['pred_labels'][0]\n",
    "                            key_rk = emotions['keyword'].get(\"Noun_keywords\")[0]\n",
    "                            keyword_rk = dict(Counter(key_rk))\n",
    "                            key_ak = emotions['keyword'].get(\"Not_Noun_keywords\")[0]\n",
    "                            keyword_ak = dict(Counter(key_ak))\n",
    "                        except:\n",
    "                            source_emo = None\n",
    "                            keyword_rk = {}\n",
    "                            keyword_ak = {}\n",
    "                            \n",
    "                        comments_dict = {\n",
    "                                        '_id': _id,\n",
    "                                        'title': post_title,\n",
    "                                        'keyword': keyword,\n",
    "                                        'date': date, #timestamp 형식의 날짜\n",
    "                                        'dt_hour': dt_hour,# %Y.%m.%d %H 형식의 날짜\n",
    "                                        'body': body,\n",
    "                                        'user_name': user_name,\n",
    "                                        'portal': \"daum\",\n",
    "                                        'url': post_link,\n",
    "                                        'emotions':source_emo,\n",
    "                                        'count_rk':keyword_rk,\n",
    "                                        'count_ak':keyword_ak\n",
    "                                    }\n",
    "\n",
    "#                         print(comments_dict)\n",
    "                except Exception as e:\n",
    "                    print(e, \"댓글 없음\")\n",
    "\n",
    "            elif soup.find(\"div\", {\"id\":\"cContentInfo\"}):\n",
    "                print(post_link)\n",
    "                if soup.find(\"div\", {\"id\":\"commentListBlock_\"}):\n",
    "                    comment_list2 = [p for p in soup.find_all(\"div\",{\"class\":\"item-reply rp_general\"})]\n",
    "                    for tag in comment_list2:\n",
    "                        user_name = tag.find('strong').text\n",
    "                        body = tag.find('div',{'class':'cont'}).text.replace(\"\\r\\n\", \" \")\n",
    "                        \n",
    "                        temp_date_li = tag.find('li', {'class':'sDateTime'}).text\n",
    "                        temp_date = temp_date_li.split(' ')[0]+' '+temp_date_li.split(' ')[1]\n",
    "\n",
    "                        date = convert_post_date(temp_date)[0]\n",
    "                        dt_hour = convert_post_date(temp_date)[1]\n",
    "                        temp_id = str(post_title + str(date) + user_name)\n",
    "                        temp_id = temp_id.encode('utf-8')\n",
    "                        h = hashlib.new('md5')\n",
    "                        h.update(temp_id)\n",
    "                        _id = h.hexdigest()  \n",
    "\n",
    "                        # 감정분석 추가\n",
    "                        try:\n",
    "                            emo_response = requests.post(emo_url, json={'text': [comment]}, timeout=5)\n",
    "                            emotions = emo_response.json()\n",
    "                            source_emo = emotions['pred_labels'][0]\n",
    "                            key_rk = emotions['keyword'].get(\"Noun_keywords\")[0]\n",
    "                            keyword_rk = dict(Counter(key_rk))\n",
    "                            key_ak = emotions['keyword'].get(\"Not_Noun_keywords\")[0]\n",
    "                            keyword_ak = dict(Counter(key_ak))\n",
    "                        except:\n",
    "                            source_emo = None\n",
    "                            keyword_rk = {}\n",
    "                            keyword_ak = {}\n",
    "                            \n",
    "                        comments_dict = {\n",
    "                                        '_id': _id,\n",
    "                                        'title': post_title,\n",
    "                                        'keyword': keyword,\n",
    "                                        'date': date, #timestamp 형식의 날짜\n",
    "                                        'dt_hour': dt_hour,# %Y.%m.%d %H 형식의 날짜\n",
    "                                        'body': body,\n",
    "                                        'user_name': user_name,\n",
    "                                        'portal': \"daum\",\n",
    "                                        'url': post_link,\n",
    "                                        'emotions':source_emo,\n",
    "                                        'count_rk':keyword_rk,\n",
    "                                        'count_ak':keyword_ak\n",
    "                                    }\n",
    "                        print(comments_dict)\n",
    "            else:\n",
    "                print(\"no 댓글\")\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d086f8d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "article-reply: 조선일보에 박수를, 이재명 기자회견장 http://blog.daum.net/bolee591/16160939\n",
      "부처님 제자는 분노의 위험을 봅니다.... 세상사,사회문제,놀이거리,연극,영화등을 보면서 마음이 휩쓸리지 않습니다... 사회현상은 사회현상일뿐 그에 끄달려가지 말고 사띠를 해야합니다...\n",
      "article-reply: {9월넷째주 시사} 화천대유, 이재명과 박근혜의 결탁?!?! http://blog.daum.net/smileru/8889414\n",
      "스마일루가 소식이 좀 늦은 것으로 보인다 일단 화천대유의 실소유주, 속칭 '쩐주'는 이미 밝혀졌고 당연히 이재명이 아니다 2015년 킨앤파트너즈라는 투자컨설팅회사가 자본금 5천만원쩌리 페이퍼컴퍼니에 가장 먼저 400~600억을 빌려줬고, 상환을 받지 않은 채 그대로 투자금으로 전환했다 2017년에는 200~300억을 추가로 붓기까지 했다. 이런 수백억에 달하는 현금은 어디서 나왔을까? 바로 최태원의 여동생 최기원이다 수많은 기사가 이미 나왔고 본인도 부인하지 않는다 당시 최태원이 구속 중이었던 것을 생각하면, 그리고 최기원의 SK 지분율을 생각하면 사실상 회장대리의 투자 결정이라고 생각해도 좋다 킨앤파트너즈의 핵심인사는 계속 교체되었지만 SK 출신이 맡았던 것도 좋은 방증이 된다 그래도 증거가 부족하다면 이른바 '개인투자자'들 자금을 특정금전신탁으로 중개한 것도 SK증권이고 가장 노른자 땅에는 SK건설이 지은 판교SK뷰 테라스가 들어섰다는 것도 알아두면 좋다 어쨌든 2015년 재벌회장 최장기간 구속 기록을 매일 경신하고 있던 최태원은 대통령의 특별사면이 간절했고 청와대 민정수석 곽상도가 놓은 다리로 최순실에 접근해 결국 원하던 광복절 특사를 받았다 곽상도는 그 보은으로 화천대유에 아들 곽병채를 끼워넣어 50억 뇌물을 받았고 기타 보험 차원에서 당시 정권의 수많은 법복귀족들이 한자리 차지했으며 박영수는 국정농단 미르/k-스포츠 재단 사건에서 '은안' 최태원을 불기소 처분해 보답했다 그러면 최태원은 또 박영수 딸 퇴직금으로 수십억을 얹아주며 보은하겠지? 한국 법조계의 일상적 구조적 영구적 부정부패 생태계의 실상이 이러하며 사회 정의에 가격을 매겨 사고파는 더러운 관계를 부끄러워하기는커녕 아름답고 인간적이라 여긴다  이재명? 이재명이 대권주자 급으로 떠오른 것은 탄핵 이후라는 점을 기억해야 한다 변호사 자격에 직접 인허가권자이긴 하지만 당시 시민운동가 출신 야당, 재선 시장은 저 당당한 법복귀족들의 '끕'에 맞지 않는다 박근혜 정권의 실세를 야당 시장이 무슨 수로 알고 끌어모았겠는가? 그런 커넥션이 있었다면 왜 대선 정국에 진작 폭로가 되지 않았겠는가?  물론 전임 시장 김대업처럼 뇌물에 환장한 인간이었으면 기꺼이 몇푼 쥐어주고 1조에 육박하는 개발이익을 전부 SK 최씨 가문과 그 밑에 모여든 판검사 떨거지들이 나눠먹었을 것이다 그러나 이재명은 시장실에 CCTV를 설치하고 뇌물을 싸들고 오는 사람들을 쫓아냈으며 토건세력에 특혜를 주지 않겠다는 강경한 입장을 취했다 결국 SK 일가는 수익 절반을 공공개발 형태로 시가 환수하는 타협안에 만족할 수밖에 없었다 화천대유, 천하동인 고위층은 이재명과 \"결탁\"하기는커녕 자기 돈을 빼앗아간 불구대천의 원수로 여긴다 이재명 그 똥고집 미치광이 때문에 황금알을 낳는 거위가 반토막이 났다고 지금도 이를 갈고 있을 것이다 그러나 재벌가의 특성상, 그리고 정황상 그룹의 비자금을 동원했을 것이기 때문에 겉으로 드러나는 것을 원하지 않고 입을 다물었던 것이다 곽상도 아들 퇴직금이 50억이라는 보도가 언제 나왔는가? 최기원이 킨앤파트너즈를 거쳐 화천대유에 400억을 부었다는 보도가 나온 다음 날이었다 이런 귀찮은 일을 피하고 싶어서 수많은 법복귀족들에게 사탕을 물려준 것인데 대선판에 말려드는 것을 막지 못한, 오히려 특검을 하자고 난리를 친 곽상도를 응징한 것이다 조선일보는 세습자본의 명을 충직히 받들어 화천대유 기사를 1면에서 5면으로 옮겼고 서서히 비중을 즐여나가고 있다 장담컨대 본선에서 화천대유는 별다른 변수가 되지 못할 것이다 대선 직전, 가장 혼란할 때 기습적으로 터뜨렸다면 모를까 (윤석열 측에서 그런 준비를 하고 있었다는 말은 있다) 너무 일찍 터져서 선후관계가 다 정리가 되어버렸으니..\n",
      "말해주신 내용이 꽤나 충격적이네요. 저도 좀 더 알아보겠습니다. 감사합니다. \n",
      "화천대유는 뭔가 전개가 웃기네요. 국민의힘에서 이재명 때릴 건수다 싶어서 마구 때렸는데  정작 던진돌이 자기들에게 돌아오고 있으니ㅋㅋ  50억이라는 금액의 임팩트가 워낙 크다보니 이재명이 구체적으로 얼마 해먹었다는 내용이 나오지 않는 이상 타격은 국민의 힘으로 쏠리거 같네요\n",
      "그러게말입니다. 그래도 이재명이 화천대유가 돈을 받을 수 있게 한 것 아니냐, 라는 포인트가 남아있긴 해서... 그 쪽의 의심을 이재명이 어떻게 벗어날 수 있을지가 또 포인트 아닌가 싶네요. \n",
      "화천대유는 파면 뭔가 나오긴 할텐데 이재명지사와 연결고리는 아마 찾아내기 힘들지 않을까... 싶습니다. 있었으면 진작 나왔겠죠. 지금까지도 \"5천억 벌어간 개꿀사업때 성남시장이었던 사람이 아무것도 없을리 없어!\"라는 정황증거 수준이라... 반대로 그 의심은 합리적 근거가 없기 때문에 벗어나기도 힘들고 결국 별 이슈 없이 비비다가 사라질 것 같습니다.\n",
      "저도 같은 생각입니다. 문제는 이게 선거에 어떤 영향을 미치냐일텐데... 그건 조금 더 지켜봐야 될 것 같습니다. 여론전이 뭐 어마무시하네요. \n",
      "dd님의 말대로라면 너무 일찍 대장동을 이재명과 엮어서 이재명을 추락시키려고 시도한 인사는 법관귀족들의 응징을 받는 것입니까? 곽상도에게 그러했듯..  그.. 처음에 이 건을 들춘 인물은 누구입니까. 전 그게 매우 궁금하네요.!\n",
      "경기일보에서 시작되긴 했었고, 보니 해당 기자가 나름 이재명이나 경기도에 대한 적대감이 큰 것 같더라구요. 하지만 그렇다고 해서 왜곡보도를 하진 않았다고 생각됩니다. 뭔가 문제가 있었음은 분명하니까요. \n",
      "article-reply: 펌]이재명 전기 민영화하려고함 http://blog.daum.net/na-1127/2547\n",
      "한전전기를 민영화한다는 말이 아닙니다 송배전 공사를 한전이 독점하고 공사비가 비싸기 때문에 송배전공사를 민간에 주겠다는 말입니다 정확한 공약은 모르지만 지금 링크한기사는 한전민영화가 아닙니다\n",
      "넘나 KTX타고 광속으로 지나가면서 봐도 민영환데요? 악질인게 IMF때 한전 민영화 얘기 나올때도 송전은 제외하고 배전 민영화였어요. 근데 이 분은 송전도 포함이네요?ㅋㅋㅋㅋㅋ 솔직히 배전은 공사건수가 많지만 진짜 쫌쫌따리고 금액과 규모는 송전이 넘사인데.. 이쪽에도  뭐 해놓은게 있으신가 싶을 정도인데.......... 예 그렇게 믿고 싶은가 본데 그냥 그렇게 사세요 본인눈에 보이는 것만 보이는 법이죠 \n",
      "님같은 분들덕분에 저런 쓰레기가 나대는겁니다 민영화 맞습니다\n",
      "세상사람들은 그걸 민영화라고 부르고 있답니다. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tc/_rwdzpmd7d7ghdk4lclpghzc0000gn/T/ipykernel_54439/2209300572.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'content_error'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"해당 게시물이 존재하지 않습니다.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9/html/parser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9/html/parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<!--\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9/html/parser.py\u001b[0m in \u001b[0;36mparse_endtag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mrawdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"</\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unexpected call to parse_endtag\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendendtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# >\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "HEADERS_LIST = \"\"\"Mozilla/5.0 Slackware/13.37 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/11.0.696.50\n",
    "Mozilla/5.0 Slackware/13.37 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/12.0.742.91\"\"\".split('\\n')\n",
    "HEADERS_LIST = [x.strip() for x in HEADERS_LIST]\n",
    "agent = random.choice(HEADERS_LIST)\n",
    "headers = {\"User-Agent\": \"{}\".format(agent)}\n",
    "\n",
    "\n",
    "# 감정분석 api 변수 저장\n",
    "emo_url = 'http://172.16.114.47:80/model-engine/social_senti_keyword/'\n",
    "\n",
    "DRIVER_PATH = '/Users/kimminho/chromedriver'\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "\n",
    "keyword = \"이재명\"\n",
    "startDate = \"20211004140224\"\n",
    "endDate = \"20211005140224\"\n",
    "\n",
    "for pageNo in range(1, 80):\n",
    "    print(pageNo)\n",
    "    target_url = f\"https://search.daum.net/search?w=blog&enc=utf8&q={keyword}&f=section&SA=daumsec&period=d&p={pageNo}&f=sectionsd={startDate}&ed={endDate}\"\n",
    "    driver.get(target_url)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    \n",
    "    if soup.find(\"span\", attrs={\"class\":\"btn_page btn_next\"}):\n",
    "        print(\"더 이상 페이지가 없습니다.\")\n",
    "        break\n",
    "        \n",
    "    if soup.find(\"div\", attrs={\"id\": \"noResult\"}):\n",
    "        print(\"검색결과 없음\", target_url)    \n",
    "        #break\n",
    "\n",
    "    posts = soup.find(\"ul\", attrs={\"class\": \"list_info mg_cont clear\"}).find_all(\"li\")\n",
    "    for post in posts:\n",
    "        \n",
    "        \n",
    "        post_link = \"\"\n",
    "        \n",
    "        post_title = post.find(\"a\", attrs={\"class\": \"f_link_b\"}).get_text()\n",
    "        post_link = post.find(\"a\", attrs={\"class\": \"f_link_b\"})['href']\n",
    "\n",
    "\n",
    "        blog_name_list = post.find_all(\"span\", attrs={\"class\": \"f_l\"})\n",
    "        blog_name = blog_name_list[0].find(\"a\", attrs={\"class\": \"f_nb\"}).get_text()\n",
    "\n",
    "        res = requests.get(post_link, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        if soup.find('div', {'class': 'content_error'}):\n",
    "            print(\"해당 게시물이 존재하지 않습니다.\")\n",
    "            break\n",
    "\n",
    "        if soup.find('div', {'id':'article-reply'}):\n",
    "            print(\"article-reply:\",post_title, post_link)\n",
    "            comment_list = [p for p in soup.find_all('div', {\"class\":\"box-content\"})]\n",
    "\n",
    "            for tag in comment_list:\n",
    "                user_name = tag.find('strong').text\n",
    "                temp_body = tag.find('p').text.replace(\"\\r\\n\", \" \")\n",
    "                if temp_body == \"비밀댓글입니다\" or temp_body == \"관리자의 승인을 기다리고 있는 댓글입니다\":\n",
    "                    continue\n",
    "                body = temp_body.replace(\"\\n\", \" \")\n",
    "                print(body)\n",
    "                \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a739a39",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1176034898.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/tc/_rwdzpmd7d7ghdk4lclpghzc0000gn/T/ipykernel_3762/1176034898.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <div class=\"item-reply rp_general\" id=\"comment17362740\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1\n",
    "article-reply: 조선일보에 박수를, 이재명 기자회견장 http://blog.daum.net/bolee591/16160939\n",
    "관리자의 승인을 기다리고 있는 댓글입니다\n",
    "부처님 제자는 분노의 위험을 봅니다.... 세상사,사회문제,놀이거리,연극,영화등을 보면서 마음이 휩쓸리지 않습니다... 사회현상은 사회현상일뿐 그에 끄달려가지 말고 사띠를 해야합니다...\n",
    "비밀댓글입니다\n",
    "관리자의 승인을 기다리고 있는 댓글입니다\n",
    "비밀댓글입니다\n",
    "article-reply: {9월넷째주 시사} 화천대유, 이재명과 박근혜의 결탁?!?! http://blog.daum.net/smileru/8889414\n",
    "스마일루가 소식이 좀 늦은 것으로 보인다 일단 화천대유의 실소유주, 속칭 '쩐주'는 이미 밝혀졌고 당연히 이재명이 아니다 2015년 킨앤파트너즈라는 투자컨설팅회사가 자본금 5천만원쩌리 페이퍼컴퍼니에 가장 먼저 400~600억을 빌려줬고, 상환을 받지 않은 채 그대로 투자금으로 전환했다 2017년에는 200~300억을 추가로 붓기까지 했다. 이런 수백억에 달하는 현금은 어디서 나왔을까? 바로 최태원의 여동생 최기원이다 수많은 기사가 이미 나왔고 본인도 부인하지 않는다 당시 최태원이 구속 중이었던 것을 생각하면, 그리고 최기원의 SK 지분율을 생각하면 사실상 회장대리의 투자 결정이라고 생각해도 좋다 킨앤파트너즈의 핵심인사는 계속 교체되었지만 SK 출신이 맡았던 것도 좋은 방증이 된다 그래도 증거가 부족하다면 이른바 '개인투자자'들 자금을 특정금전신탁으로 중개한 것도 SK증권이고 가장 노른자 땅에는 SK건설이 지은 판교SK뷰 테라스가 들어섰다는 것도 알아두면 좋다 어쨌든 2015년 재벌회장 최장기간 구속 기록을 매일 경신하고 있던 최태원은 대통령의 특별사면이 간절했고 청와대 민정수석 곽상도가 놓은 다리로 최순실에 접근해 결국 원하던 광복절 특사를 받았다 곽상도는 그 보은으로 화천대유에 아들 곽병채를 끼워넣어 50억 뇌물을 받았고 기타 보험 차원에서 당시 정권의 수많은 법복귀족들이 한자리 차지했으며 박영수는 국정농단 미르/k-스포츠 재단 사건에서 '은안' 최태원을 불기소 처분해 보답했다 그러면 최태원은 또 박영수 딸 퇴직금으로 수십억을 얹아주며 보은하겠지? 한국 법조계의 일상적 구조적 영구적 부정부패 생태계의 실상이 이러하며 사회 정의에 가격을 매겨 사고파는 더러운 관계를 부끄러워하기는커녕 아름답고 인간적이라 여긴다  이재명? 이재명이 대권주자 급으로 떠오른 것은 탄핵 이후라는 점을 기억해야 한다 변호사 자격에 직접 인허가권자이긴 하지만 당시 시민운동가 출신 야당, 재선 시장은 저 당당한 법복귀족들의 '끕'에 맞지 않는다 박근혜 정권의 실세를 야당 시장이 무슨 수로 알고 끌어모았겠는가? 그런 커넥션이 있었다면 왜 대선 정국에 진작 폭로가 되지 않았겠는가?  물론 전임 시장 김대업처럼 뇌물에 환장한 인간이었으면 기꺼이 몇푼 쥐어주고 1조에 육박하는 개발이익을 전부 SK 최씨 가문과 그 밑에 모여든 판검사 떨거지들이 나눠먹었을 것이다 그러나 이재명은 시장실에 CCTV를 설치하고 뇌물을 싸들고 오는 사람들을 쫓아냈으며 토건세력에 특혜를 주지 않겠다는 강경한 입장을 취했다 결국 SK 일가는 수익 절반을 공공개발 형태로 시가 환수하는 타협안에 만족할 수밖에 없었다 화천대유, 천하동인 고위층은 이재명과 \"결탁\"하기는커녕 자기 돈을 빼앗아간 불구대천의 원수로 여긴다 이재명 그 똥고집 미치광이 때문에 황금알을 낳는 거위가 반토막이 났다고 지금도 이를 갈고 있을 것이다 그러나 재벌가의 특성상, 그리고 정황상 그룹의 비자금을 동원했을 것이기 때문에 겉으로 드러나는 것을 원하지 않고 입을 다물었던 것이다 곽상도 아들 퇴직금이 50억이라는 보도가 언제 나왔는가? 최기원이 킨앤파트너즈를 거쳐 화천대유에 400억을 부었다는 보도가 나온 다음 날이었다 이런 귀찮은 일을 피하고 싶어서 수많은 법복귀족들에게 사탕을 물려준 것인데 대선판에 말려드는 것을 막지 못한, 오히려 특검을 하자고 난리를 친 곽상도를 응징한 것이다 조선일보는 세습자본의 명을 충직히 받들어 화천대유 기사를 1면에서 5면으로 옮겼고 서서히 비중을 즐여나가고 있다 장담컨대 본선에서 화천대유는 별다른 변수가 되지 못할 것이다 대선 직전, 가장 혼란할 때 기습적으로 터뜨렸다면 모를까 (윤석열 측에서 그런 준비를 하고 있었다는 말은 있다) 너무 일찍 터져서 선후관계가 다 정리가 되어버렸으니..\n",
    "말해주신 내용이 꽤나 충격적이네요. 저도 좀 더 알아보겠습니다. 감사합니다. \n",
    "화천대유는 뭔가 전개가 웃기네요. 국민의힘에서 이재명 때릴 건수다 싶어서 마구 때렸는데  정작 던진돌이 자기들에게 돌아오고 있으니ㅋㅋ  50억이라는 금액의 임팩트가 워낙 크다보니 이재명이 구체적으로 얼마 해먹었다는 내용이 나오지 않는 이상 타격은 국민의 힘으로 쏠리거 같네요\n",
    "그러게말입니다. 그래도 이재명이 화천대유가 돈을 받을 수 있게 한 것 아니냐, 라는 포인트가 남아있긴 해서... 그 쪽의 의심을 이재명이 어떻게 벗어날 수 있을지가 또 포인트 아닌가 싶네요. \n",
    "화천대유는 파면 뭔가 나오긴 할텐데 이재명지사와 연결고리는 아마 찾아내기 힘들지 않을까... 싶습니다. 있었으면 진작 나왔겠죠. 지금까지도 \"5천억 벌어간 개꿀사업때 성남시장이었던 사람이 아무것도 없을리 없어!\"라는 정황증거 수준이라... 반대로 그 의심은 합리적 근거가 없기 때문에 벗어나기도 힘들고 결국 별 이슈 없이 비비다가 사라질 것 같습니다.\n",
    "저도 같은 생각입니다. 문제는 이게 선거에 어떤 영향을 미치냐일텐데... 그건 조금 더 지켜봐야 될 것 같습니다. 여론전이 뭐 어마무시하네요. \n",
    "dd님의 말대로라면 너무 일찍 대장동을 이재명과 엮어서 이재명을 추락시키려고 시도한 인사는 법관귀족들의 응징을 받는 것입니까? 곽상도에게 그러했듯..  그.. 처음에 이 건을 들춘 인물은 누구입니까. 전 그게 매우 궁금하네요.!\n",
    "경기일보에서 시작되긴 했었고, 보니 해당 기자가 나름 이재명이나 경기도에 대한 적대감이 큰 것 같더라구요. 하지만 그렇다고 해서 왜곡보도를 하진 않았다고 생각됩니다. 뭔가 문제가 있었음은 분명하니까요. \n",
    "article-reply: 펌]이재명 전기 민영화하려고함 http://blog.daum.net/na-1127/2547\n",
    "한전전기를 민영화한다는 말이 아닙니다 송배전 공사를 한전이 독점하고 공사비가 비싸기 때문에 송배전공사를 민간에 주겠다는 말입니다 정확한 공약은 모르지만 지금 링크한기사는 한전민영화가 아닙니다\n",
    "넘나 KTX타고 광속으로 지나가면서 봐도 민영환데요? 악질인게 IMF때 한전 민영화 얘기 나올때도 송전은 제외하고 배전 민영화였어요. 근데 이 분은 송전도 포함이네요?ㅋㅋㅋㅋㅋ 솔직히 배전은 공사건수가 많지만 진짜 쫌쫌따리고 금액과 규모는 송전이 넘사인데.. 이쪽에도  뭐 해놓은게 있으신가 싶을 정도인데.......... 예 그렇게 믿고 싶은가 본데 그냥 그렇게 사세요 본인눈에 보이는 것만 보이는 법이죠 \n",
    "님같은 분들덕분에 저런 쓰레기가 나대는겁니다 민영화 맞습니다\n",
    "세상사람들은 그걸 민영화라고 부르고 있답니다. \n",
    "article-reply: {9월첫째주 시사} 이재명 과반 왜? 윤석열 고발사주 등 http://blog.daum.net/smileru/8889408\n",
    "박용진은 입법가로서는 실적과 전망 모두 밝지만 조직의 리더나 행정가로서는 미지수 아닌가 본선에 올라와서 김두관을 제친 것만 해도 대단한 성과이다 후보 중 가장 젊은 축에 속하니 계속 도전하면 능력을 발휘할 기회가 분명 있을 것이다  다른 한편으로는 추미애의 선전이 주목할 만하다 이재명 이낙연 구도에는 미치지 못하겠지만 캠프의 의원 한 명이 없는 안타까울 정도의 조직력으로 정세균과 3위 다툼을 하고 있다 앞으로 바람에 따라 계속 역할을 할 수 있지 않을까\n",
    "박용진 후보는 물론 그렇죠. 어차피 시작인 것이니까요... 그냥 좀 더 의미있는 성과를 보여주었으면 좋았을텐데 하는 아쉬움이 있네요.   추미애의 경우는 윤석열의 반대급부 아닐까 싶긴 합니다. 향후 행보가 궁금해지네요.  \n",
    "이낙연/이재명을 지지하던 사람이 자기 지지하는 사람이 떨어졌다고 국힘을 뽑는다...? 잠깐 격앙되서 그런 말을 하더라도 실제로 그런 선택을 하는 사람이 과연 얼마나 될까 싶네요  윤석열 건은.... 뭐 제대로 밝혀지긴 어렵지 않으려나요 진실이더라도 법 잘 아는 사람들이니 발뺄 구석은 어떻게든 만들어놨을듯 한데\n",
    "진실 밝혀지긴 어렵겠죠. 민주당도 애매한채로 남겨두려 하지 않을까 싶어요. \n",
    "개인적으로 이재명 지사 정책은 너무 극좌라 비공감하지만 저랑 똑같은 흙수저 출신 아동학대 피해자라서 지지합니다.   노무현이나 이명박 홍준표 같은 대통령이 흙이라 하는데 자식이 마음껏 공부할 수 있게 응원하는게 무슨 흙입니까.  아버지는 노름질하고 자식 학교도 안 보내고 14살에 소년공.. 이런 사람이 되어 자식을 노후대비 보험 노예로 생각하는 막장 부모를 신성시하는 유교나치 문화 좀 뿌리뽑았으면 하네요\n",
    "이재명의 정책이 그런 쪽으로도 있는지는 모르겠네요. 뭐 아마 아동학대나 관련해서 뭔가 정책을 내 놓긴 하지 않을까 싶습니다. \n",
    "2\n",
    "article-reply: “단군이래 최대 5503억원 공익환수” 이재명 주장 따져보니... http://blog.daum.net/heegryu/4694\n",
    "article-reply: 대장동과 이재명 http://blog.daum.net/peter0516/1574\n",
    "article-reply: 화천대유, 천화동인, 성남의 뜰, 이재명 아웃! http://blog.daum.net/dlscjs47/815\n",
    "article-reply: 대법 연구관들 “이재명 유죄” 냈다가… 권순일 “무죄” 주장에, 추가... http://blog.daum.net/heegryu/4790\n",
    "article-reply: 대장동 게이트의 몸통은 이재명입니다!| http://blog.daum.net/yunsamolovegroup/86\n",
    "article-reply: 목숨 걸고 공부한 이재명 http://blog.daum.net/bolee591/16160936\n",
    "무엇에 목숨을 거는 것이 엄청 중요합니다. 돈과 권력, 가족을 위해서가 아니라 사람과 중생을 위해 몸을 바치는 부처같은 정치인이 필요한거죠. 이것도 욕심인줄 압니다만 잘 선택해야 합니다. 국운이 정해지니까요... 화이팅 대한민국. \n",
    "article-reply: 화천대유 이재명. http://blog.daum.net/choong9854205/6241\n",
    "민주당놈들 은근 슬쩍 국힘당 에 책임 전가...ㅋㅋㅋㅉㅉㅉ.\n",
    "3\n",
    "article-reply: 귀태(鬼胎) 이재명 http://blog.daum.net/cahiker/2019\n",
    "article-reply: 이재명 지사ㅡ 수박론 http://blog.daum.net/k800012/5900\n",
    "article-reply: 망나니 이재명의 배경 http://blog.daum.net/psk1236/2461\n",
    "article-reply: 인권변호사도 가짜였던 이재명 http://blog.daum.net/na-1127/2605\n",
    "어휴 진짜 저런 인간을 지지하는 사람들 정신 챙겨라~~~~!!!특히 현 쩜.오구 당댚 이하 지도부들 창피한줄 아셔라  !!!\n",
    "블로그 주인인데요 영구정지 당해서 다른곳에서 찾아뵙도록 하겠습니다 ㅠㅜ 그럼 이만...........명박그네때도 안당한걸 당해봐서 참 허탈하고 어이가 없을뿐....... 여러분그인간은 절대 아니에요 모두들 안녕히...\n",
    "이 글도 곧 삭제될까요?\n",
    "그냥  한  마디로     '사기꾼'\n",
    "인생자체가 사기네요\n",
    "청문회때 이미 검증된 이낙연으로 가야합니다 국힘이 화천대유 후속으로 다른거 또 파고있대요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b9c3eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd 2021.09.06 09:58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'박용진은 입법가로서는 실적과 전망 모두 밝지만 조직의 리더나 행정가로서는 미지수 아닌가본선에 올라와서 김두관을 제친 것만 해도 대단한 성과이다후보 중 가장 젊은 축에 속하니 계속 도전하면 능력을 발휘할 기회가 분명 있을 것이다다른 한편으로는 추미애의 선전이 주목할 만하다이재명 이낙연 구도에는 미치지 못하겠지만캠프의 의원 한 명이 없는 안타까울 정도의 조직력으로정세균과 3위 다툼을 하고 있다앞으로 바람에 따라 계속 역할을 할 수 있지 않을까'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tag.find('strong').text, tag.find('span').text, tag.find('p').text.replace('\\n',''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
